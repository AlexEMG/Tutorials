{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's import some packages including TensorFlow (which has Keras as a subset)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's load the MNIST data:\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to one hot encoding \n",
    "print(\"(Example) target:\",y_train[0])\n",
    "y_test=tf.keras.utils.to_categorical(y_test)\n",
    "y_train=tf.keras.utils.to_categorical(y_train)\n",
    "print(\"(Example) target after one hot encoding:\",y_train[0])\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read number of samples and dimensions of images\n",
    "numsamples,nx,ny=np.shape(x_train)\n",
    "plt.imshow(x_train[5,:,:],cmap='gray') #What does that do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's plot a few example images\n",
    "plt.figure(figsize=(20,5))\n",
    "for i in range(30):\n",
    "    fig = plt.subplot(2, 15, i + 1)\n",
    "    index=np.random.randint(numsamples)\n",
    "    fig.imshow(x_train[index,:,:],cmap='binary') \n",
    "    fig.set_xticks(())\n",
    "    fig.set_yticks(())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('MNIST.png')\n",
    "\n",
    "#Can you identify all characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up a model, consisting of:\n",
    "\n",
    "# a flatten layer, which \"flattens\" the 28 x 28 pixels into 784 pixels, then\n",
    "# connect it densely to 512 nodes;\n",
    "# these nodes are densely connected to 10 softmax nodes!\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we configure the model, i.e. which optimizer, loss, and metric (for displaying performance) \n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(0.001), \n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "'''\n",
    "- Loss function —This measures how accurate the model is during training. We want to minimize this function to \"steer\" the model in the right direction.\n",
    "- Optimizer —This is how the model is updated based on the data it sees and its loss function.\n",
    "- Metrics —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified.\n",
    "'''\n",
    "out=model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "train_loss, train_acc =model.evaluate(x_train, y_train)\n",
    "print('Training accuracy:', train_acc)\n",
    "\n",
    "test_loss, test_acc =model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit returns a history object, here is how we can use it to plot the learning curve\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(range(len(out.history['acc'])),out.history['acc'])\n",
    "plt.xticks(range(len(out.history['acc'])))\n",
    "#plt.xlabel('Trainings batch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(range(len(out.history['acc'])),out.history['loss'])\n",
    "plt.xticks(range(len(out.history['acc'])))\n",
    "plt.xlabel('Trainings batch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's built a convolutional network that achieves > 99% accuracy!\n",
    "# This is based on: http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(1000, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Conv net expects a 4 tensor, which is why we rearrange the data.\n",
    "x_train=x_train.reshape(numsamples,28,28,1)\n",
    "x_test=x_test.reshape(np.shape(x_test)[0],28,28,1)\n",
    "\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(0.01), \n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "out=model.fit(x_train, y_train, epochs=10)\n",
    "\n",
    "train_loss, train_acc =model.evaluate(x_train, y_train)\n",
    "print('Training accuracy:', train_acc)\n",
    "\n",
    "test_loss, test_acc =model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with Adam optimizer is even faster... Adam is a first-order gradient-based optimization of\n",
    "# stochastic objective functions, paper: https://arxiv.org/abs/1412.6980\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "out=model.fit(x_train, y_train, epochs=3)\n",
    "\n",
    "print('Done training, evaluating network..')\n",
    "train_loss, train_acc =model.evaluate(x_train, y_train)\n",
    "print('Training accuracy:', train_acc)\n",
    "\n",
    "test_loss, test_acc =model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And on to Fashion MNIST... \n",
    "#Source: https://www.tensorflow.org/tutorials/keras/basic_classification\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "#Scale values to unit range:\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few examples from this data set\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up a sequential model, consisting of:\n",
    "# a flatten layer, the resulting 784 pixels are densely connected to 128 nodes;\n",
    "# these nodes are densely connected to 10 softmax nodes!\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we choose which optimizer, loss, and metric (for displaying performance)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train:\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and evaluate on train and test set...\n",
    "\n",
    "train_loss, train_acc =model.evaluate(train_images, train_labels)\n",
    "print('Training accuracy:', train_acc)\n",
    "test_loss, test_acc =model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)\n",
    "# Predict labels for all test images\n",
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some auxiliary functions for displaying predictions and examples:\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  \n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "  \n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "  predictions_array, true_label = predictions_array[i], true_label[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "  plt.ylim([0, 1]) \n",
    "  predicted_label = np.argmax(predictions_array)\n",
    " \n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first X test images, their predicted label, and the true label\n",
    "# Color correct predictions in blue, incorrect predictions in red\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_image(i, predictions, test_labels, test_images)\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_value_array(i, predictions, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DLCdependencies]",
   "language": "python",
   "name": "conda-env-DLCdependencies-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
